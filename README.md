# üí≥ Credit Card Fraud Detection (Kafka + Spark Streaming + FastAPI)

An end-to-end, streaming-style fraud detection pipeline built around the classic **Credit Card Fraud Detection** dataset (highly imbalanced).  
It simulates real-time transactions via **Kafka**, scores them using a **FastAPI** inference service (XGBoost model), and processes the stream using **Spark Structured Streaming**, persisting results into a **SQLite** database.

---

## ‚ú® What‚Äôs inside

- **Model training**: Train an XGBoost classifier on the Kaggle dataset
- **Inference API**: FastAPI service exposes `POST /predict`
- **Streaming pipeline**:
  - Producer streams transactions into Kafka topic `fraud_topic`
  - Spark reads from Kafka, calls the API for predictions, and stores results to SQLite

---

### Dataset Link
[Credit Card Fraud Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

---

## üß± Tech Stack

- Python, Pandas, NumPy
- XGBoost + scikit-learn
- FastAPI + Uvicorn
- Apache Kafka + Zookeeper
- PySpark Structured Streaming
- SQLite (for storing predictions)

---

## üìÅ Directory Structure

```
Credit-Card-Fraud-Detection-main/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                # FastAPI inference service (/predict)
‚îÇ   ‚îú‚îÄ‚îÄ fraud_model.pkl        # Trained model (generated by training script)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py         # Train XGBoost + export fraud_model.pkl
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îú‚îÄ‚îÄ producer.py            # Kafka producer that streams CSV rows
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ processor/
‚îÇ   ‚îú‚îÄ‚îÄ spark_job.py           # Spark Structured Streaming consumer + API caller + SQLite writer
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ docker-compose.yml         # Brings up Zookeeper, Kafka, API, Producer, Spark Processor
```


> **Dataset file expected**: `producer/creditcard.csv`

---

## ‚úÖ Prerequisites

### Option A (recommended): Docker
- Docker + Docker Compose installed

### Option B: Local Python (optional)
- Python 3.10+
- You‚Äôll need Java for Spark if running `processor/` locally (Docker handles this already)

---

## üì¶ Dataset Setup

This project uses the common Kaggle dataset **‚ÄúCredit Card Fraud Detection‚Äù**.

1. Download the dataset from Kaggle.
2. Place the CSV here: `producer/creditcard.csv`


The scripts expect the file name to be exactly `creditcard.csv`.

---

## üöÄ Quick Start (Docker Compose)

From the project root:

```bash
docker compose up --build
````

This starts:

* `zookeeper` on `2181`
* `kafka` on `9092`
* `fraud-api` on `8000`
* `kafka-producer` (streams records into Kafka topic `fraud_topic`)
* `spark-processor` (consumes stream, calls API, writes SQLite DB)

### Check the API

Open:

* `http://localhost:8000/docs` (Swagger UI)

---

## üß† Train / Retrain the Model (optional)

A pretrained `api/fraud_model.pkl` is already included, but you can retrain.

### Local training

From the project root:

```bash
pip install -r model/requirements.txt
python model/train_model.py
```

This will:

* Load `producer/creditcard.csv`
* Train an XGBoost model
* Save the model to:

```
api/fraud_model.pkl
```

> If you retrain, rebuild the API container:

```bash
docker compose up --build
```

---

## üîå API Usage

### Endpoint

`POST /predict`

### Request body format

The model expects numeric features:

* `V1` ... `V28`
* `Amount`

Example:

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "V1": -1.359807, "V2": -0.072781, "V3": 2.536346, "V4": 1.378155,
    "V5": -0.338321, "V6": 0.462388, "V7": 0.239599, "V8": 0.098698,
    "V9": 0.363787, "V10": 0.090794, "V11": -0.551600, "V12": -0.617801,
    "V13": -0.991390, "V14": -0.311169, "V15": 1.468177, "V16": -0.470401,
    "V17": 0.207971, "V18": 0.025791, "V19": 0.403993, "V20": 0.251412,
    "V21": -0.018307, "V22": 0.277838, "V23": -0.110474, "V24": 0.066928,
    "V25": 0.128539, "V26": -0.189115, "V27": 0.133558, "V28": -0.021053,
    "Amount": 149.62
  }'
```

### Response

```json
{ "fraud": false }
```

---

## üóÉÔ∏è Where are predictions stored?

The Spark processor writes a SQLite DB file named:

```
fraud_predictions.db
```

Since the Spark job runs inside the `spark-processor` container, the DB is created **inside the container filesystem** by default.

### Copy the DB out of the container

```bash
docker cp spark-processor:/app/fraud_predictions.db ./fraud_predictions.db
```

You can then inspect it locally (example using sqlite3):

```bash
sqlite3 fraud_predictions.db
.tables
SELECT * FROM fraud_transactions LIMIT 5;
```

---

## üß™ Running Components Individually (local, non-Docker)

### 1) Start the API

```bash
cd api
pip install -r requirements.txt
uvicorn app:app --host 0.0.0.0 --port 8000
```

### 2) Train model (optional)

```bash
cd model
pip install -r requirements.txt
python train_model.py
```

### 3) Producer (requires Kafka reachable at `kafka:9092` in the current code)

If running locally, you‚Äôll need to update `bootstrap_servers` in `producer.py` (e.g., `localhost:9092`) and ensure Kafka is accessible.

---

## üõ†Ô∏è Troubleshooting

* **`FileNotFoundError: creditcard.csv`**
  Ensure `producer/creditcard.csv` exists and is named exactly `creditcard.csv`.

* **Kafka not ready / connection errors**
  Sometimes Kafka takes a moment to be ready after containers start. Re-run:

  ```bash
  docker compose restart producer spark-processor
  ```

* **Port conflict on 8000**
  Something else is using port 8000. Either stop it or change the port mapping in `docker-compose.yml`.

---

## üìå Notes / Current Assumptions

* Kafka topic name is hardcoded as: `fraud_topic`
* Spark calls the API at: `http://api:8000/predict` (Docker Compose service DNS)
* SQLite is used as a lightweight sink for predictions

---

## üìÑ License

MIT

```
```
